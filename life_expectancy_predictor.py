# -*- coding: utf-8 -*-
"""Life Expectancy Predictor

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12RlBajO3gbRnhUjDTCw7GSQeY2JsF711

Data Cleaning
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, roc_auc_score

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import keras_tuner as kt

import xgboost as xgb

from scipy.stats import randint

# Load dataset
dataset = pd.read_csv('Survival_Data.csv')

#Drop undesired features
dataset = dataset.drop(['studyid', 'site', 'strength_comfort_religion', 'petition_prayer_health', 'intercessory_prayers_health'], axis = 1)

# Separate the dataset into numerical and categorical features
numerical_features = dataset.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_features = dataset.select_dtypes(exclude=['int64', 'float64']).columns.tolist()


# Use median imputation on the numerical features for missing values
dataset[numerical_features] = dataset[numerical_features].fillna(dataset[numerical_features].median())

# Use mode imputation on the categorical features for missing values
for col in categorical_features:
    dataset[col] = dataset[col].fillna(dataset[col].mode().iloc[0])


# Convert binary categorical variables ('Yes'/'No') into 1/0
binary_columns = [col for col in categorical_features if dataset[col].nunique() == 2]
for col in binary_columns:
    unique_values = dataset[col].unique()
    if set(unique_values) == {'Yes', 'No'}:
        dataset[col] = dataset[col].replace({'Yes': 1, 'No': 0})
    elif set(unique_values) == {'Not impaired', 'Impaired'}:
        dataset[col] = dataset[col].replace({'Impaired': 1, 'Not impaired': 0})
    else:
        mapping = {unique_values[0]: 1, unique_values[1]: 0}
        dataset[col] = dataset[col].map(mapping)


# Remaining categorical variables (multi-category)
multi_category_columns = [col for col in categorical_features if col not in binary_columns]


# One-hot encode categorical variables
encoder = OneHotEncoder(drop='first', sparse_output=False)
encoded_categorical = encoder.fit_transform(dataset[multi_category_columns])
encoded_categorical_df = pd.DataFrame(encoded_categorical, columns=encoder.get_feature_names_out(multi_category_columns))


# Drop original categorical columns and merge encoded ones
dataset = dataset.drop(multi_category_columns, axis=1)
dataset = pd.concat([dataset, encoded_categorical_df], axis=1)

dataset = dataset.rename(columns={
    'agecat_>=55-<65': 'agecat_55_to_65',
    'agecat_>=65': 'agecat_65_plus'
})


# Separate features and target
X = dataset.drop('died_2_year', axis=1)
y = dataset['died_2_year']


# Scale numerical features
scaler = StandardScaler()
X[numerical_features] = scaler.fit_transform(X[numerical_features])


# Split into Train (80%), Validation (10%), and Test (10%)
X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_rest, y_rest, test_size=0.5, random_state=42)


param_dist = {
    'n_estimators': randint(50, 100),
    'max_depth': randint(3, 7),
    'min_samples_leaf': randint(5, 20),
    'min_samples_split': randint(5, 20),
}

random_search = RandomizedSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_distributions=param_dist,
    n_iter=20,
    cv=5,
    random_state=42,
    scoring='accuracy',
    n_jobs=-1
)

# Fit model
random_search.fit(X_train, y_train)

# Get best model based on the params
best_rfc = random_search.best_estimator_

# Evaluate model
train_score = np.mean(cross_val_score(best_rfc, X_train, y_train, cv=5))
val_score = best_rfc.score(X_val, y_val)
test_score = best_rfc.score(X_test, y_test)

# Output results
print("Best Parameters:", random_search.best_params_)
print("Train Score:", train_score)
print("Validation Score:", val_score)
print("Test Score:", test_score)


y_pred = best_rfc.predict(X_test)
report = classification_report(y_test, y_pred)
print("Classification Report:\n", report)


X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_rest, y_rest, test_size=0.5, random_state=42)


# Build Complex Neural Network Model
def build_model():
    model = Sequential()
    
    # Input Layer
    model.add(Dense(512, input_shape=(X_train.shape[1],), activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.3))
    
    # Hidden Layers
    model.add(Dense(256, activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.3))
    
    model.add(Dense(128, activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.3))
    
    model.add(Dense(64, activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.3))
    
    # Output Layer
    model.add(Dense(1, activation='sigmoid'))
    
    # Compile Model
    optimizer = Adam(learning_rate=0.001)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
    
    return model


# Instantiate Model
model = build_model()

history = model.fit(
    X_train, y_train, 
    epochs=100, 
    batch_size=32,
    validation_data=(X_val, y_val),
    callbacks=[
        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)
    ],
    verbose=1
)


# Evaluate Model
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc:.4f}")


# Plot Loss Curve
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training vs Validation Loss')
plt.legend()

# Plot Accuracy Curve
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training vs Validation Accuracy')
plt.legend()

plt.show()


# Predict on test set
y_pred = (model.predict(X_test) > 0.5).astype(int)

# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap='Blues')
plt.title('Confusion Matrix')
plt.show()


X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_rest, y_rest, test_size=0.5, stratify=y_rest, random_state=42)


base_model = xgb.XGBClassifier(random_state=42, eval_metric="logloss", use_label_encoder=False)


# Define Parameter Grid
param_grid = {
    'n_estimators': [50, 100],  
    'max_depth': [3, 5, 7],            
    'learning_rate': [0.01, 0.1, 0.3], 
    'subsample': [0.6, 0.8, 1.0],      
    'colsample_bytree': [0.6, 0.8, 1.0],
    'reg_alpha': [0.1, 0.5],    
    'reg_lambda': [0.1, 0.5]    
}

# Grid Search with StratifiedKFold (5 Folds)
grid_search = GridSearchCV(
    base_model, param_grid,
    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
    scoring='accuracy', n_jobs=-1, verbose=2
)

# Perform Grid Search
grid_search.fit(X_train, y_train)

# Print Best Parameters & Score
print("Best Parameters:", grid_search.best_params_)
print("Best Cross-Validation Accuracy:", grid_search.best_score_)


# Convert Data to DMatrix
dtrain = xgb.DMatrix(X_train, label=y_train)
dval = xgb.DMatrix(X_val, label=y_val)
dtest = xgb.DMatrix(X_test, label=y_test)

# Train Best Model with Early Stopping
params = grid_search.best_params_
params.update({'objective': 'binary:logistic', 'eval_metric': 'logloss', 'random_state': 42})

evals = [(dtrain, 'train'), (dval, 'val')]
best_xgb = xgb.train(
    params, dtrain,
    num_boost_round=1000,  # Large number, but early stopping will stop it earlier
    evals=evals,
    early_stopping_rounds=10,  # Stop if no improvement after 10 rounds
    verbose_eval=True
)

# Make Predictions
y_pred = (best_xgb.predict(dtest) > 0.5).astype(int)  # Convert probabilities to 0/1
y_pred_proba = best_xgb.predict(dtest)

# Evaluate
test_acc = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {test_acc:.4f}")


X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_rest, y_rest, test_size=0.5, stratify=y_rest, random_state=42)

param_grid = {
    'C': [0.1, 1, 3, 5, 10],         # Regularization Strength
    'solver': ['liblinear', 'lbfgs'],  # Solver choice
    'max_iter': [100, 200, 500]        # Iteration limits
}

# Initialize Base Model
base_model = LogisticRegression(penalty='l2', random_state=42)

# Grid Search with Stratified 5-Fold Cross Validation
grid_search = GridSearchCV(
    base_model, param_grid,
    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
    scoring='accuracy', n_jobs=-1, verbose=2
)
grid_search.fit(X_train, y_train)

# Print Best Parameters & Score
print("Best Parameters:", grid_search.best_params_)
print("Best Cross-Validation Accuracy:", grid_search.best_score_)


# Train Best Model on Full Training Data
best_lr = grid_search.best_estimator_
best_lr.fit(X_train, y_train)

# Evaluate on Validation Set
val_pred = best_lr.predict(X_val)
val_acc = accuracy_score(y_val, val_pred)
print(f"Validation Accuracy: {val_acc:.4f}")

# Evaluate on Test Set
test_pred = best_lr.predict(X_test)
test_acc = accuracy_score(y_test, test_pred)
print(f"Test Accuracy: {test_acc:.4f}")

# Detailed Classification Report
print("\nClassification Report:\n", classification_report(y_test, test_pred))


X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_rest, y_rest, test_size=0.5, stratify=y_rest, random_state=42)


# Define SVC model
svc = SVC(random_state=42)

# Hyperparameter tuning using GridSearchCV
param_grid = {
    'C': [0.1, 1, 10], 
    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],
    'gamma': ['scale', 'auto']
}

grid_search = GridSearchCV(svc, param_grid, cv=StratifiedKFold(n_splits=5), scoring='accuracy', verbose=1, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Best model from Grid Search
best_svc = grid_search.best_estimator_

# Train Best Model on Training Data
best_svc.fit(X_train, y_train)

# Evaluate on Validation Set
y_val_pred = best_svc.predict(X_val)
val_accuracy = accuracy_score(y_val, y_val_pred)
print(f"Validation Accuracy: {val_accuracy:.5f}")

# Final Test Evaluation
y_test_pred = best_svc.predict(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
print(f"Test Accuracy: {test_accuracy:.5f}")

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred))


